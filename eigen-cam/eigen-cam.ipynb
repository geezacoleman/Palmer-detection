{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "50a82c4e",
   "metadata": {},
   "source": [
    "# Eigen Class Activation Mapping (CAM)\n",
    "\n",
    "This notebook has been adapted from [Jacob Gil's excellent tutorial](https://jacobgil.github.io/pytorch-gradcam-book/EigenCAM%20for%20YOLO5.html). It allows the visualisation of the model attention in each image, to better understand why it makes specific decisions around detection and class.\n",
    "\n",
    "We will use the PAGS8-trained YOLOv5-S and YOLOv5-X models on 1280 x 1280 images from fold 0.\n",
    "\n",
    "A key consideration when using Eigen-CAM is the target layer within the model. We're going to use the second last layer within the dection layer. But you can play around with this value and the model to see how it changes.\n",
    "\n",
    "This is done in:\n",
    "\n",
    "    ```python\n",
    "    model.model.model.model[-2]\n",
    "    ```\n",
    "\n",
    "<img src=\"https://github.com/geezacoleman/Palmer-detection/assets/51358498/a929c774-f570-4eb2-95c4-c70cb507de8b\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c97e4e02e0082027"
  },
  {
   "cell_type": "markdown",
   "source": [
    "This package needs to load custom models, and as such requires a local YOLOv5 repository. Either clone the repository from the official Ultralytics repository or if you're using the Palmer-detection repository, supply the path to the yolov5 folder.\n",
    "\n",
    "Install the requirements.txt file in that directory."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "925d2cd8c64e9f72"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%cd '/content/drive/MyDrive/Colab Notebooks/Palmer-detection/yolov5'\n",
    "!pip install -r requirements.txt"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c6daac021cd5a22f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Install the grad-cam package to run the Eigen CAM tool."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "468bb7f957d422a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install grad-cam"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ad8415929d4976e1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')\n",
    "import os    \n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from pytorch_grad_cam import EigenCAM\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image, scale_cam_image\n",
    "from PIL import Image\n",
    "\n",
    "COLORS = np.random.uniform(0, 255, size=(80, 3))\n",
    "\n",
    "def parse_detections(results):\n",
    "    detections = results.pandas().xyxy[0]\n",
    "    detections = detections.to_dict()\n",
    "    boxes, colors, names = [], [], []\n",
    "\n",
    "    for i in range(len(detections[\"xmin\"])):\n",
    "        confidence = detections[\"confidence\"][i]\n",
    "        if confidence < 0.2:\n",
    "            continue\n",
    "        xmin = int(detections[\"xmin\"][i])\n",
    "        ymin = int(detections[\"ymin\"][i])\n",
    "        xmax = int(detections[\"xmax\"][i])\n",
    "        ymax = int(detections[\"ymax\"][i])\n",
    "        name = detections[\"name\"][i]\n",
    "        category = int(detections[\"class\"][i])\n",
    "        color = COLORS[category]\n",
    "\n",
    "        boxes.append((xmin, ymin, xmax, ymax))\n",
    "        colors.append(color)\n",
    "        names.append(name)\n",
    "    return boxes, colors, names\n",
    "\n",
    "\n",
    "def draw_detections(boxes, colors, names, img):\n",
    "    for box, color, name in zip(boxes, colors, names):\n",
    "        xmin, ymin, xmax, ymax = box\n",
    "        cv2.rectangle(\n",
    "            img,\n",
    "            (xmin, ymin),\n",
    "            (xmax, ymax),\n",
    "            color, \n",
    "            2)\n",
    "\n",
    "        cv2.putText(img, name, (xmin, ymin - 5),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2,\n",
    "                    lineType=cv2.LINE_AA)\n",
    "    return img\n",
    "\n",
    "\n",
    "def renormalize_cam_in_bounding_boxes(boxes, colors, names, image_float_np, grayscale_cam, output, image_name='test', model='v5X'):\n",
    "    \"\"\"Normalize the CAM to be in the range [0, 1] \n",
    "    inside every bounding boxes, and zero outside of the bounding boxes. \"\"\"\n",
    "    crops = []\n",
    "    renormalized_cam = np.zeros(grayscale_cam.shape, dtype=np.float32)\n",
    "    for x1, y1, x2, y2 in boxes:\n",
    "        renormalized_cam[y1:y2, x1:x2] = scale_cam_image(grayscale_cam[y1:y2, x1:x2].copy())    \n",
    "\n",
    "    renormalized_cam = scale_cam_image(renormalized_cam)\n",
    "    eigencam_image_renormalized = show_cam_on_image(image_float_np, renormalized_cam, use_rgb=False)\n",
    "    for i, ((x1, y1, x2, y2), name) in enumerate(zip(boxes, names)):\n",
    "        cropped_image = eigencam_image_renormalized[y1:y2, x1:x2]\n",
    "        output_path = os.path.join(output, f\"{image_name}_crop_{model}_{name}_{i}.png\")\n",
    "        cv2.imwrite(output_path, cropped_image)\n",
    "        crops.append(cropped_image)\n",
    "                                                 \n",
    "    image_with_bounding_boxes = draw_detections(boxes, colors, names, eigencam_image_renormalized)\n",
    "    return image_with_bounding_boxes, crops"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "51616028a2d78575"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%cd '/content/drive/MyDrive/Palmer-detection/eigen-cam'\n",
    "YOLO_V5_PATH = '/content/drive/MyDrive/Palmer-detection/yolov5' # path to the repository if using a local model file\n",
    "MODEL_FILE = '/content/drive/MyDrive/Palmer-detection/eigen-cam/yolov5s_pags8.pt' # path to the trained model\n",
    "IMAGE_FILE = 'test1.jpg'"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9f237a7dca452e79"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True) # this will load a pretrained model\n",
    "model = torch.hub.load('ultralytics/yolov5', 'custom', path=MODEL_FILE, force_reload=True) # this will load a custom model\n",
    "model.eval()\n",
    "model.cpu()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fcc903b0d8600d37"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load the image."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "56c1b69015120e5d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "image = cv2.imread(IMAGE_FILE)\n",
    "image = cv2.resize(image, (1280, 1280))\n",
    "rgb_img = cv2.cvtColor(image.copy(), cv2.COLOR_BGR2RGB) \n",
    "image = np.float32(image) / 255\n",
    "transform = transforms.ToTensor()\n",
    "tensor = transform(image).unsqueeze(0)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8233d3741856aeea"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Run the model on the image and display the detections.\n",
    "\n",
    "![image](https://github.com/geezacoleman/Palmer-detection/assets/51358498/e76167a3-f9ef-4d4b-8cc4-c5ffbeca7e76)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a3fd70e72407a4c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results = model([rgb_img.copy()])\n",
    "boxes, colors, names = parse_detections(results)\n",
    "detections = draw_detections(boxes, colors, names, rgb_img.copy())\n",
    "Image.fromarray(detections)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8bca2614dae094de"
  },
  {
   "cell_type": "markdown",
   "id": "00deab61",
   "metadata": {},
   "source": [
    "Now let's create our CAM model and run it on the image. We have selected the last layer in the model detection head. Feel free to play around with this value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "target_layers = [model.model.model.model[-1].m[-1]]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7fd474cfa62a936e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2133d608",
   "metadata": {},
   "outputs": [],
   "source": [
    "cam = EigenCAM(model, target_layers, use_cuda=True)\n",
    "grayscale_cam = cam(tensor)[0, :, :]\n",
    "cam_image = show_cam_on_image(image, grayscale_cam, use_rgb=True)\n",
    "Image.fromarray(cam_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "![image](https://github.com/geezacoleman/Palmer-detection/assets/51358498/5d051d35-5102-41a0-af8d-788e13c57b3c)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e6151fdbe8f021d1"
  },
  {
   "cell_type": "markdown",
   "id": "22493307",
   "metadata": {},
   "source": [
    "This heatmap highlights the Palmer amaranth in the centre of the image, clearly focusing on the central inflorescence. We can further refine this by focusing the heatmap on the detected area only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4aadd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_DIRECTORY = '/content/drive/MyDrive/Colab Notebooks/palmer_amaranth/eigen-cam/bbox_crops'\n",
    "\n",
    "renormalized_cam_image, crops = renormalize_cam_in_bounding_boxes(boxes, colors, names, image, grayscale_cam, SAVE_DIRECTORY)\n",
    "Image.fromarray(crops[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can also iterate over an entire directory of images and save the renormalised bounding box heatmaps to a directory. This is the process we used in the paper."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4b86a50066f40958"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5186e325",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "INPUT_DIR = '/content/drive/MyDrive/Colab Notebooks/palmer_amaranth/eigen-cam/images'\n",
    "\n",
    "for image_file in tqdm(os.listdir(INPUT_DIR)):\n",
    "  image_path = os.path.join(INPUT_DIR, image_file)\n",
    "\n",
    "  image = cv2.imread(image_path)\n",
    "  image = cv2.resize(image, (1280, 1280))\n",
    "  rgb_img = cv2.cvtColor(image.copy(), cv2.COLOR_BGR2RGB) \n",
    "  image = np.float32(image) / 255\n",
    "  transform = transforms.ToTensor()\n",
    "  tensor = transform(image).unsqueeze(0)\n",
    "\n",
    "  results = model([rgb_img.copy()])\n",
    "  boxes, colors, names = parse_detections(results)\n",
    "\n",
    "  cam = EigenCAM(model, target_layers, use_cuda=True)\n",
    "  grayscale_cam = cam(tensor)[0, :, :]\n",
    "\n",
    "  renormalized_cam_image, crops = renormalize_cam_in_bounding_boxes(boxes, colors, names, image, grayscale_cam, SAVE_DIRECTORY, os.path.basename(image_path), model='v5x')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
